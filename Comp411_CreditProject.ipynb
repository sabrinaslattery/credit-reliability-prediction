{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sabrinaslattery/credit-reliability-prediction/blob/main/Comp411_CreditProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2XObaFhtgbC"
   },
   "source": [
    "# Determining Default Payment from 23 Feature Variables\n",
    "\n",
    "### Researcher: Sabrina Slattery\n",
    "\n",
    "The problem given is a classification problem and will treated as such. The goal is to find the best model that can precisely determine whether or not a costumer will default on their credit card payments. The data given in the data set will be cleaned, analyzed, and run through four different models to see which of the different tested models give the highest degree of precision. \n",
    "\n",
    "This project is best treated as a supervised machine learning problem. The main reason for choosing this type over unsupervised learning is that the data is labeled.\n",
    "\n",
    "#### Feature Variables Explained:\n",
    "\n",
    "> X1: Amount of the given credit\n",
    ">\n",
    "> X2: Gender (1 = male; 2 = female)\n",
    ">\n",
    "> X3: Education (1 = graduate school; 2 = university; 3 = high school; \n",
    "    4 = others)\n",
    ">\n",
    "> X4: Marital status (1 = married; 2 = single; 3 = others)\n",
    ">\n",
    "> X5: Age\n",
    ">\n",
    "> X6-X11 = The repayment status in September through April, 2005. \n",
    "\n",
    "   >>(The measurement scale for the repayment status is: -1 = pay duly; \n",
    "   >>1 = payment delay for one month; \n",
    "   >>\n",
    "   >>2 = payment delay for two months; \n",
    "   >>. . .; \n",
    "   >>8 = payment delay for eight months; \n",
    "   >>\n",
    "   >>9 = payment delay for nine months and above.)\n",
    "\n",
    "> X12-X17 = Amount of bill statement in September through April, 2005.\n",
    ">\n",
    "> X18-X23 = Amount of previous payments made in September through April, 2005.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_4QMO5wyBuB"
   },
   "source": [
    "### Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "teWmcWGMvMB7",
    "outputId": "f2d253de-c6fc-47c9-f567-51506892ce09"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0b8d0f21f5f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminant_analysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split, ShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Perceptron\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, r2_score, precision_score, recall_score, f1_score, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_YDa31g80eN"
   },
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTXEmJt5zGx4"
   },
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "DoRCZjWLvP8z",
    "outputId": "506935d8-aff4-4bcb-e74f-59bf90c6dab9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/franklin-univ-data-science/data/master/credit_default.csv')\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gewi67gIFJe"
   },
   "source": [
    "#### Rename Labels for Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "jdAs7sQJv-zJ",
    "outputId": "7519a491-ccdb-4d02-e415-8a1ad4781a48"
   },
   "outputs": [],
   "source": [
    "df.columns = ['ID', 'Credit', 'Gender', 'Education', 'Mar_Stat', 'Age',\n",
    "              'Stat_905', 'Stat_805', 'Stat_705', 'Stat_605', 'Stat_505', 'Stat_405',\n",
    "              'Bill_905', 'Bill_805', 'Bill_705', 'Bill_605', 'Bill_505', 'Bill_405',\n",
    "              'Prev_905', 'Prev_805', 'Prev_705', 'Prev_605', 'Prev_505', 'Prev_405',\n",
    "              'Default_Pay']\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74zUiW-aIOin"
   },
   "source": [
    "# Cleaning the Data\n",
    "### Finding nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUgkOXRwxmrw",
    "outputId": "84fedab5-5d34-49c4-84a2-2828fa894132"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZDFGPX5-Du0"
   },
   "source": [
    "There are no null values in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FzYUyUrsmCq"
   },
   "source": [
    "### Visualize Feature Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4QB05K8w03d"
   },
   "source": [
    "The float data types are analyzed using catplots and scatterplots. Then, using factorplots, the discrete data types are analyzed and feature importance is determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMHnq7FDIgD-"
   },
   "source": [
    "#### Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "VZHsu_DZvxA7",
    "outputId": "9951e17e-1b1b-4dd6-b4b3-52ac2b4313b2"
   },
   "outputs": [],
   "source": [
    "credit = sns.catplot(x=\"Default_Pay\",y=\"Credit\",data=df,kind=\"violin\", height=5, aspect=3)\n",
    "credit.set_ylabels(\"Credit\")\n",
    "credit.set_xticklabels(['No','Yes'])\n",
    "credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPWPNLN_thTQ"
   },
   "source": [
    "We see that the number of people defaulting their payments is much more concentrated on those with lower credit, possibly meaning that people with low credit are more likely to default their payments as opposed to those with higher credit as seen in the 'no' group.\n",
    "\n",
    "\"Credit\" has at least one datapoint with a value of 1.00e6, which drastically changes the distribution of credit for those who do not default their payments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A34AazHQIjtZ"
   },
   "source": [
    "#### Repayment Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UD4He541usiG",
    "outputId": "0ebc125e-ed81-457e-a4ad-6cc6887867e8"
   },
   "outputs": [],
   "source": [
    "cols = ['Stat_905', 'Stat_805', 'Stat_705', 'Stat_605', 'Stat_505', 'Stat_405', 'Default_Pay']\n",
    "\n",
    "sns.pairplot(df[cols], height=2.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrSGQ5oxvD_x"
   },
   "source": [
    "For all 6 repayment status features, there are one or more values of -2, which is an illogical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G0dhHFSIvKX"
   },
   "source": [
    "#### Bill Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7yHYzDTKvRAu",
    "outputId": "f21f2529-dffb-44a5-bad1-98c7c21099b9"
   },
   "outputs": [],
   "source": [
    "cols = ['Bill_905', 'Bill_805', 'Bill_705', 'Bill_605', 'Bill_505', 'Bill_405', 'Default_Pay']\n",
    "\n",
    "sns.pairplot(df[cols], height=2.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_DJ8zfcvlNp"
   },
   "source": [
    "For all 6 bill amount features, there is at least one value = 1.00e6, which can be considered an outlier and will need to be or addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xjb1DPQiI_Gs"
   },
   "source": [
    "#### Previous Payment Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ss3zSgYnv6ri",
    "outputId": "4add23bc-698c-4773-b918-f360f3fcbf3d"
   },
   "outputs": [],
   "source": [
    "cols = ['Prev_905', 'Prev_805', 'Prev_705', 'Prev_605', 'Prev_505', 'Prev_405', 'Default_Pay']\n",
    "\n",
    "sns.pairplot(df[cols], height=2.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ls6x7w8BxB_x"
   },
   "source": [
    "Here, all 6 previous payment amounts have values between 0.8e6 and 1.0e6. Because of the presence of these values, we can assume that the values of 1.0e6 in the bill amount features are justified. There is the possibility of error, and because these values are outliers of the dataset they will be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsfBVE410hVJ"
   },
   "source": [
    "## Comparing Feature and Target Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM5QIch7xRjP"
   },
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679
    },
    "id": "J3SAWjCsveVM",
    "outputId": "ae3485bf-27d1-40fb-9593-1d1c73e678c7"
   },
   "outputs": [],
   "source": [
    "count = sns.countplot(x=\"Gender\",data=df)\n",
    "sex = sns.catplot(x=\"Gender\",y=\"Default_Pay\",data=df,kind=\"violin\")\n",
    "sex.set_ylabels(\"Default Pay\")\n",
    "sex.set_xticklabels(['Male (1)', 'Female (2)'])\n",
    "count, sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6NjeonIvjM0"
   },
   "source": [
    "The gender representation in this dataset leans towards female (2), but are generally similar when it comes to default payment. Female clients are slightly more likely to not default on their payment, while male clients are slightly more likely to default on their payment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYreGQ_YxRjQ"
   },
   "source": [
    "### Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679
    },
    "id": "exaIGJiaulRH",
    "outputId": "432fe40e-1c92-48d9-ccd6-c00aeff9d8bf"
   },
   "outputs": [],
   "source": [
    "count = sns.countplot(x=\"Education\",data=df)\n",
    "ed = sns.catplot(x=\"Education\",y=\"Default_Pay\",data=df,kind=\"bar\")\n",
    "ed.set_ylabels(\"Default Pay\")\n",
    "count, ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBH7q3GMurcK"
   },
   "source": [
    "This dataset includes mostly college (1) and graduate-level (2) clients. However, high school (value=3) level clients are more likely to default their payments.\n",
    "\n",
    "The feature variable \"Education\" includes values = 0, 5, and 6. These are illogical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 681
    },
    "id": "SL0o55fxuWB_",
    "outputId": "bec73346-6b24-4c44-b594-64250c72c090"
   },
   "outputs": [],
   "source": [
    "count = sns.countplot(x=\"Mar_Stat\",data=df)\n",
    "mar = sns.catplot(x=\"Mar_Stat\",y=\"Default_Pay\",data=df,kind=\"bar\")\n",
    "mar = mar.set_ylabels(\"Default Pay\")\n",
    "count, mar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3271TGJIuecy"
   },
   "source": [
    "Our data is roughly evenly split between married (1) and single (2) clients, and there is no clear correlation between Mar_Stat and Default_Pay with these values. We see that although other (3) is a very small proportion of the data, this group is more likely to vary in Default_Pay.\n",
    "\n",
    "The feature variable \"Mar_Stat\" includes value = 0, which is an illogical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v5fD7mWp2Iry",
    "outputId": "3ce586f6-1a62-4c64-b2f6-171eb85c813b"
   },
   "outputs": [],
   "source": [
    "g = sns.kdeplot(df[\"Age\"][(df[\"Default_Pay\"] == 0) & (df[\"Age\"].notnull())], color=\"Red\", shade = True)\n",
    "g = sns.kdeplot(df[\"Age\"][(df[\"Default_Pay\"] == 1) & (df[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\n",
    "g.set_xlabel(\"Age\")\n",
    "g.set_ylabel(\"Frequency\")\n",
    "g = g.legend([\"No\",\"Yes\"])\n",
    "\n",
    "age = sns.catplot(x=\"Age\",y=\"Default_Pay\",data=df, kind=\"bar\", height=6, aspect=2.5)\n",
    "age = age.set_ylabels(\"Default Pay\")\n",
    "g, age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0QbCPQd2jVg"
   },
   "source": [
    "The age range is roughly 20 to 80, with the majority of data comprised of ages around 25-35 years. There is a lot more variance in default pay in ages 60 and older, which can possibly be attributed to the technology learning curve in older generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6woTTJN5nIi"
   },
   "source": [
    "### Consolidating and Labeling Illogical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_oxbqfonYYP"
   },
   "source": [
    "#### Outliers that will be re-labeled or added to the measurement scales of their respective feature, because of illogical data: \n",
    "\n",
    ">Repayment Status values that are not integers -1 and greater\n",
    "\n",
    "Illogical values in Repayment Status include value = -2, which will be considered an \"other\" on the measurement scale.\n",
    "\n",
    ">Marital Status values that are not integers 1, 2 or 3\n",
    "\n",
    "Illogical values in Marital Status include all values above or below the existing measurement scale. This only includes value = 0, which will be considered an \"unknown\" and will be given a value of 4.\n",
    "\n",
    ">Education Level values that are not integers 1, 2, 3, or 4\n",
    "\n",
    "Illogical values in Education Level includes all values above or below the existing measurement scale. All these values less than 1 or greater than 4 will be re-valued at value = 5 and considered \"unknown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7Yir_siLHMI"
   },
   "outputs": [],
   "source": [
    "#Marital Status\n",
    "mar_stat_drop = df.loc[(df['Mar_Stat'] < 1) | (df['Mar_Stat'] > 3)]\n",
    "df.loc[df['Mar_Stat'] == 0, ['Mar_Stat']] = 4\n",
    "\n",
    "#Education\n",
    "education_drop = df.loc[(df['Education'] < 1) | (df['Education'] > 4)]\n",
    "\n",
    "df.loc[df['Education'] == 0, ['Education']] = 5\n",
    "df.loc[df['Education'] == 5, ['Education']] = 5\n",
    "df.loc[df['Education'] == 6, ['Education']] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "id": "-EoXOOTA6A7U",
    "outputId": "7bb263ee-1f95-4cfb-872c-f4469484e9db"
   },
   "outputs": [],
   "source": [
    "mar = sns.catplot(x=\"Mar_Stat\",y=\"Default_Pay\",data=df,kind=\"bar\", aspect=2)\n",
    "mar.set_ylabels(\"Default Pay\")\n",
    "mar.set_xticklabels(['Married (1)','Single (2)','Other (3)','Unknown (4)'])\n",
    "\n",
    "ed = sns.catplot(x=\"Education\",y=\"Default_Pay\",data=df,kind=\"bar\", height=5, aspect=2)\n",
    "ed.set_ylabels(\"Default Pay\")\n",
    "ed.set_xticklabels(['Graduate School (1)','College (2)','High School (3)','Other (4)', 'Unknown (5)'])\n",
    "\n",
    "mar, ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpIbI5hU6H-6"
   },
   "source": [
    "After consolidating illogical values to a single \"other\" measurement, the distributions of education and marital status have been compressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wN5hIRSXMDLP"
   },
   "source": [
    "## Balancing the Target Variable (Default Payment)\n",
    "The target variable is imbalanced between 'no' values (77.88%) and 'yes' values (22.12%). To make sure the data is balanced, an oversampling technique will be used. SMOTE oversampling will increase the likelihood of overfitting, therefore the data will be run through more than one prediction model and evaluated using learning curves to determine overfitting/underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "Sr0hvgaTydLZ",
    "outputId": "527caca7-adb9-44a3-8d31-d973c959f503"
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:-1].values\n",
    "y = df.iloc[:, -1:].values.flatten()\n",
    "\n",
    "sns.set(font_scale=1.2)\n",
    "tv = sns.countplot(y)\n",
    "tv.set_xticklabels(['No','Yes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7T-r_oWwNXSm"
   },
   "source": [
    "### SMOTE Using Imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "CzQK3M8XNL6p",
    "outputId": "649094bf-1cff-4529-f508-88e382e731ae"
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:-1].values\n",
    "y = df.iloc[:, -1:].values.flatten()\n",
    "\n",
    "sm = SMOTE(random_state=123)\n",
    "X_sm, y_sm = sm.fit_resample(X,y)\n",
    "              \n",
    "print('Original dataset shape', Counter(y))\n",
    "print('Original dataset shape', Counter(y_sm))\n",
    "X = X_sm\n",
    "y = y_sm.ravel()# removes DataConversionError warning\n",
    "\n",
    "sns.set(font_scale=1.2)\n",
    "tv = sns.countplot(y)\n",
    "tv.set_xticklabels(['No','Yes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeYu7yx3SGUB"
   },
   "source": [
    "The target variable data is now balanced. Later, I will check to make sure the chosen models are not overfit because of oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exwmjq16L-x5"
   },
   "source": [
    "End Data Pre-Processing\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JN-mDXCz3uH"
   },
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oew1OXrl2Ftg"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=123)\n",
    "df.columns = ['ID', 'Credit', 'Gender', 'Education', 'Mar_Stat', 'Age',\n",
    "              'Stat_905', 'Stat_805', 'Stat_705', 'Stat_605', 'Stat_505', 'Stat_405',\n",
    "              'Bill_905', 'Bill_805', 'Bill_705', 'Bill_605', 'Bill_505', 'Bill_405',\n",
    "              'Prev_905', 'Prev_805', 'Prev_705', 'Prev_605', 'Prev_505', 'Prev_405',\n",
    "              'Default_Pay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZyRGVI59IDg"
   },
   "source": [
    "## Feature Scaling and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yt5Bphs89M7F"
   },
   "outputs": [],
   "source": [
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "var = pca.explained_variance_ratio_\n",
    "print(\"Top 2 Components:\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f9B2ymZ9hky",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GdwU3Du9QSj",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We will evaluate feature importance using a heatmap and evaluate the top two important features using linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "ekyrZIzszO-Q",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "6f0857c0-7f87-4cfc-c9f5-c517b13bbba9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(23,23))\n",
    "sns.set(font_scale=1.8)\n",
    "cm1 = np.corrcoef(df[df.columns[1:]].values.T)\n",
    "hm1 = sns.heatmap(cm1,\n",
    "                 cbar=True,\n",
    "                 annot=True,\n",
    "                 square=True,\n",
    "                 fmt='.2f',\n",
    "                 annot_kws={'size': 16},\n",
    "                 yticklabels=df.columns[1:],\n",
    "                 xticklabels=df.columns[1:])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaLTgg1F4xXR",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "In this heatmap, we see that the greatest correlation to default payment is within the client's status. \"Stat_905\" and \"Stat_805\" have the greatest correlation coefficients with default payment with 0.32 and 0.26, respectively.\n",
    "\n",
    "However, we also see that \"Stat_905\" and \"Stat_805\" correlate to eachother moderately high at 0.67. Considering the other correlations of the status features, these two features are the most likely to be correlated with deafult payment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64pZ_Ix43a5Y"
   },
   "source": [
    "# Prediction Modeling\n",
    "Decision Tree and Random Forest algorithms were chosen because of time, memory, and processing constraints. I also believe that tree-based algorithms will give the best overall and most easily comprehensible result for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od1k_C7s-viQ"
   },
   "source": [
    "## Hyperparameter Searching\n",
    "\n",
    "I will not be using accuracy scores because the original target variable (i.e. before SMOTE) is widely imbalanced. To further reduce the likelihood of inaccurate scoring because of imbalances, larger penalties will be assigned to wrong predictions on the minority ('yes') class using a class_weight for each algorithm that allows the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OECR9Q4LGj_"
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKnuFXTRLF9M",
    "outputId": "9341ab54-a875-4171-c876-cc4109c563ce"
   },
   "outputs": [],
   "source": [
    "pipe_dclf = make_pipeline(DecisionTreeClassifier(class_weight='balanced'))\n",
    "pipe_dclf.fit(X_train,y_train)\n",
    "y_train_pred = pipe_dclf.predict(X_train)\n",
    "\n",
    "param_grid = dict(decisiontreeclassifier__criterion=['gini', 'entropy'],\n",
    "                  decisiontreeclassifier__max_depth=[20,24,28,32,36,40]\n",
    "                  )\n",
    "\n",
    "pipe_dclf_gs = GridSearchCV(estimator=pipe_dclf, \n",
    "                  param_grid=param_grid,\n",
    "                  scoring=None,\n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "pipe_dclf_gs.fit(X_train,y_train)\n",
    "\n",
    "print('Best Criterion:', pipe_dclf_gs.best_estimator_.get_params()['decisiontreeclassifier__criterion'])\n",
    "print('Best max_depth:', pipe_dclf_gs.best_estimator_.get_params()['decisiontreeclassifier__max_depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxZiyTHRLZmf"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zvvqi263eHx",
    "outputId": "62ca960d-7428-43d6-e234-8bac81e1cf2d"
   },
   "outputs": [],
   "source": [
    "pipe_rf = make_pipeline(RandomForestClassifier(class_weight='balanced'))\n",
    "pipe_rf.fit(X_train,y_train)\n",
    "y_train_pred = pipe_rf.predict(X_train)\n",
    "\n",
    "param_grid = dict(decisiontreeclassifier__criterion=['gini', 'entropy'],\n",
    "                  decisiontreeclassifier__max_depth=[20,24,28,32,36,40]\n",
    "                  )\n",
    "\n",
    "pipe_dclf_gs = GridSearchCV(estimator=pipe_dclf, \n",
    "                  param_grid=param_grid,\n",
    "                  scoring=None,\n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "pipe_dclf_gs.fit(X_train,y_train)\n",
    "\n",
    "print('Best Criterion:', pipe_dclf_gs.best_estimator_.get_params()['decisiontreeclassifier__criterion'])\n",
    "print('Best max_depth:', pipe_dclf_gs.best_estimator_.get_params()['decisiontreeclassifier__max_depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZpYtJyTUGBS"
   },
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj6VBloZUMDY"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHPk_zfaX1HK"
   },
   "source": [
    "#### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "JJm_tmeOdri6",
    "outputId": "af199c75-726f-4cf8-bd7b-f04d244f1629"
   },
   "outputs": [],
   "source": [
    "pipe_rf = make_pipeline(RandomForestClassifier(criterion='entropy', max_depth=36, class_weight='balanced'))\n",
    "pipe_rf.fit(X_train,y_train)\n",
    "\n",
    "plot_learning_curve(pipe_rf, 'Random Forest Learning Curve', X_train, y_train, \n",
    "                    axes=None, ylim=None, cv=10, n_jobs=-1, \n",
    "                    train_sizes=np.linspace(.1, 1.0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHGuL8jYeZE7"
   },
   "source": [
    "The training score here shows that the training model is very overfit, producing a perfect score while the CV score returns a score of ~0.93."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZ8rK_3BX426"
   },
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "M301bRcmduEa",
    "outputId": "c505a3ee-f174-4f68-f52f-fe640f5b44e3"
   },
   "outputs": [],
   "source": [
    "pipe_dclf = make_pipeline(DecisionTreeClassifier(criterion='gini', max_depth=28, class_weight='balanced'))\n",
    "pipe_dclf.fit(X_train,y_train)\n",
    "\n",
    "plot_learning_curve(pipe_dclf, 'Decision Tree Learning Curve', X_train, y_train, \n",
    "                    axes=None, ylim=None, cv=10, n_jobs=-1, \n",
    "                    train_sizes=np.linspace(.1, 1.0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZw2wDbYedxB"
   },
   "source": [
    "The training score here shows that the training model is very overfit, producing a perfect score while the CV score returns a score of ~0.88.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPUa4O_9Nhe6"
   },
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TPSky1yxRjf"
   },
   "source": [
    "The evaluation metric that I preferred to use is f1 scoring. I chose this metric because it is specifically used for evaluating binary targets. I also used precision scoring and recall scoring because it is good to evaluate in comparison to f1 scoring. Additionally, I used a make_scorer in conjunction with the GridSearchCV to make sure that the imbalanced target variable data was taken into account (an extra precaution on top of running the target variable through SMOTE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01BLDpxqXc1l"
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRC4mTN8kNeH",
    "outputId": "39a50849-7703-48ed-9219-4123a8fd1bd0"
   },
   "outputs": [],
   "source": [
    "y_pred = pipe_rf.predict(X_test)\n",
    "y_test_pred = pipe_rf.predict(X_test)\n",
    "\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g16_70vpxRjg"
   },
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2fooptCjpoj",
    "outputId": "6f2fd597-87b7-4876-f352-ae5cc7100615"
   },
   "outputs": [],
   "source": [
    "y_pred = pipe_rf.predict(X_test)\n",
    "y_test_pred = pipe_rf.predict(X_test)\n",
    "\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lb5ctcQI9vOv"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sAfBN5LxkN8"
   },
   "source": [
    "- What is your model's results? Is it good? Do you have any concerns?\n",
    "  - The results of testing are relatively high, which would indicate a good model for this scenario; however, because I oversampled the target data, it cannot reliably be said that these results are accurate. I concluded from the learning curves and k-fold cv models that the models are overfit. Therefore, more intensive algorithms (i.e. SVM, NaiveBayes) would create a better prediction model given less time and memory constraints.\n",
    "\n",
    "  - Given more time, it would be appropriate to collect more data so that the target variable is not imbalanced. If that is not an option, better sampling techniques could be explored. This would hopefully address overfitting in future models. Additionally, future research should look into more memory- and time-intensive algorithms such as SVM because they are likely to deliver more accurate modeling than the tree-based models.\n",
    "  \n",
    "  - Overall, this model could be a good candidate for predicting whether or not a client will default on their credit card payments."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6FzYUyUrsmCq",
    "jMHnq7FDIgD-",
    "A34AazHQIjtZ",
    "1G0dhHFSIvKX",
    "Xjb1DPQiI_Gs",
    "AsfBVE410hVJ",
    "_sAfBN5LxkN8"
   ],
   "include_colab_link": true,
   "name": "Comp411_CreditProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
